{"code":"(window.webpackJsonp=window.webpackJsonp||[]).push([[11],{281:function(v,_,t){\"use strict\";t.r(_);var e=t(13),a=Object(e.a)({},(function(){var v=this,_=v._self._c;return _(\"ContentSlotsDistributor\",{attrs:{\"slot-key\":v.$parent.slotKey}},[_(\"h2\",{attrs:{id:\"hmms\"}},[_(\"a\",{staticClass:\"header-anchor\",attrs:{href:\"#hmms\"}},[v._v(\"#\")]),v._v(\" HMMs\")]),v._v(\" \"),_(\"blockquote\",[_(\"p\",[v._v(\"隐式马尔科夫模型：Hidden Markov Models\")]),v._v(\" \"),_(\"p\",[v._v(\"最常用的贝叶斯模型\")])]),v._v(\" \"),_(\"p\",[v._v(\"通过结果推断中间某一过程条件概率分布\")]),v._v(\" \"),_(\"p\",[v._v(\"条件概率：\"),_(\"code\",[v._v(\"P(x|y) = P(x,y)/P(y)\")])]),v._v(\" \"),_(\"p\",[v._v(\"产品规则：\"),_(\"code\",[v._v(\"P(x|y)P(y) = P(x,y)\")])]),v._v(\" \"),_(\"p\",[v._v(\"链规则：\"),_(\"code\",[v._v(\"P(X1...Xn) = P(X1)P(X2|X1)P(X3|X1,X2)...\")])]),v._v(\" \"),_(\"p\",[v._v(\"独立：\"),_(\"code\",[v._v(\"P(x,y) = P(x)P(y)\")])]),v._v(\" \"),_(\"p\",[v._v(\"条件独立：\"),_(\"code\",[v._v(\"P(x,y|z) = P(x|z)P(y|z)\")])]),v._v(\" \"),_(\"p\",[v._v(\"在隐马尔可夫链中，我们通常关注观察对象暗含的隐式序列\")]),v._v(\" \"),_(\"ul\",[_(\"li\",[v._v(\"语音解码\")]),v._v(\" \"),_(\"li\",[v._v(\"机器人坐标感知\")]),v._v(\" \"),_(\"li\",[v._v(\"推荐系统\")]),v._v(\" \"),_(\"li\",[v._v(\"医疗监控\")])]),v._v(\" \"),_(\"h3\",{attrs:{id:\"markov-models\"}},[_(\"a\",{staticClass:\"header-anchor\",attrs:{href:\"#markov-models\"}},[v._v(\"#\")]),v._v(\" Markov Models\")]),v._v(\" \"),_(\"blockquote\",[_(\"p\",[v._v(\"马尔可夫模型\")])]),v._v(\" \"),_(\"p\",[v._v(\"MDPs：马尔可夫决定过程\")]),v._v(\" \"),_(\"ul\",[_(\"li\",[_(\"p\",[v._v(\"a set of states s\")])]),v._v(\" \"),_(\"li\",[_(\"p\",[v._v(\"a set of actions a\")])]),v._v(\" \"),_(\"li\",[_(\"p\",[v._v(\"a transition function T(s, a, s')\")]),v._v(\" \"),_(\"ul\",[_(\"li\",[_(\"p\",[v._v(\"Probability that a from s leads to s', called P(s'| s, a)\")]),v._v(\" \"),_(\"p\",[v._v(\"在s状态执行a行为到达s'的代价\")])]),v._v(\" \"),_(\"li\",[_(\"p\",[v._v(\"Also called the model or the dynamics\")]),v._v(\" \"),_(\"p\",[v._v(\"不同于搜索，这个后继函数有很多个，如在每个地点都可以向东南西北移动\")])])])]),v._v(\" \"),_(\"li\",[_(\"p\",[v._v(\"a reward function R(s, a, s')\")]),v._v(\" \"),_(\"ul\",[_(\"li\",[_(\"p\",[v._v(\"sometimes just R(s) or R(s')\")]),v._v(\" \"),_(\"p\",[v._v(\"奖惩制度，有时只取决起点或终点\")])])])]),v._v(\" \"),_(\"li\",[_(\"p\",[v._v(\"a start state\")])]),v._v(\" \"),_(\"li\",[_(\"p\",[v._v(\"maybe a terminal state\")])])]),v._v(\" \"),_(\"p\",[v._v(\"基础条件独立：Basic Conditional Independence\")]),v._v(\" \"),_(\"p\",[v._v(\"基于现在，过去和未来是独立的；每个步骤只取决于上一各步骤；这被称为一阶马尔可夫属性\")]),v._v(\" \"),_(\"p\",[v._v(\"可以用贝叶斯网或有限状态自动机来表示一个马尔可夫模型\")]),v._v(\" \"),_(\"p\",[v._v(\"贝叶斯网络：\")]),v._v(\" \"),_(\"div\",{staticClass:\"language- line-numbers-mode\"},[_(\"pre\",{pre:!0,attrs:{class:\"language-text\"}},[_(\"code\",[v._v(\"X1———X2———X3———X4\\n\")])]),v._v(\" \"),_(\"div\",{staticClass:\"line-numbers-wrapper\"},[_(\"span\",{staticClass:\"line-number\"},[v._v(\"1\")]),_(\"br\")])]),_(\"h4\",{attrs:{id:\"mini-forward-algorithm\"}},[_(\"a\",{staticClass:\"header-anchor\",attrs:{href:\"#mini-forward-algorithm\"}},[v._v(\"#\")]),v._v(\" Mini-Forward Algorithm\")]),v._v(\" \"),_(\"p\",[v._v(\"正向模拟：当我们要知道马尔可夫链中第k个元素的概率时，且已知第一个元素概率，可以使用递归的方式，即k-1获得任一元素的条件概率，最终获得一整个概率序列\")]),v._v(\" \"),_(\"ul\",[_(\"li\",[v._v(\"这一结果也可以从第一个元素开始，进行变量消除得到\")])]),v._v(\" \"),_(\"p\",[v._v(\"当k->∞，P(Xk)将收敛到一个固定值，如同抛硬币收敛到0.5\")]),v._v(\" \"),_(\"p\",[v._v(\"从平均概率开始，这一状态称为静止分布，当没有证据加入时，不管如何迭代概率分布始终不变\")]),v._v(\" \"),_(\"p\",[v._v(\"当加入条件，不断迭代，变量消除，概率分布偏移\")]),v._v(\" \"),_(\"h4\",{attrs:{id:\"fixed-distribution\"}},[_(\"a\",{staticClass:\"header-anchor\",attrs:{href:\"#fixed-distribution\"}},[v._v(\"#\")]),v._v(\" Fixed distribution\")]),v._v(\" \"),_(\"p\",[v._v(\"固定分布：即马尔科夫链长度为无限时其值不再变动时的分布\")]),v._v(\" \"),_(\"p\",[v._v(\"有时可由线性方程组解出，如有概率分布表，X0表示第一天天气，X1表示第二天天气\")]),v._v(\" \"),_(\"table\",[_(\"thead\",[_(\"tr\",[_(\"th\",[v._v(\"X0\")]),v._v(\" \"),_(\"th\",[v._v(\"X1\")]),v._v(\" \"),_(\"th\",[v._v(\"P(X1|X0)\")])])]),v._v(\" \"),_(\"tbody\",[_(\"tr\",[_(\"td\",[v._v(\"sun\")]),v._v(\" \"),_(\"td\",[v._v(\"sun\")]),v._v(\" \"),_(\"td\",[v._v(\"0.9\")])]),v._v(\" \"),_(\"tr\",[_(\"td\",[v._v(\"sun\")]),v._v(\" \"),_(\"td\",[v._v(\"rain\")]),v._v(\" \"),_(\"td\",[v._v(\"0.1\")])]),v._v(\" \"),_(\"tr\",[_(\"td\",[v._v(\"rain\")]),v._v(\" \"),_(\"td\",[v._v(\"sun\")]),v._v(\" \"),_(\"td\",[v._v(\"0.3\")])]),v._v(\" \"),_(\"tr\",[_(\"td\",[v._v(\"rain\")]),v._v(\" \"),_(\"td\",[v._v(\"rain\")]),v._v(\" \"),_(\"td\",[v._v(\"0.7\")])])])]),v._v(\" \"),_(\"p\",[v._v(\"可得无穷时关于第二天天气\"),_(\"code\",[v._v(\"P∞(x2)\")]),v._v(\"的线性方程组\")]),v._v(\" \"),_(\"ul\",[_(\"li\",[_(\"code\",[v._v(\"P∞(sun) = 0.9P∞(sun)+0.3P∞(rain)\")])]),v._v(\" \"),_(\"li\",[_(\"code\",[v._v(\"P∞(rain) = 0.1P∞(sun)+0.7P∞(rain)\")])])]),v._v(\" \"),_(\"p\",[v._v(\"可得：\"),_(\"code\",[v._v(\"P(sun) = 3P(rain)\")])]),v._v(\" \"),_(\"p\",[v._v(\"又因为：\"),_(\"code\",[v._v(\"P(sun)+P(rain) = 1\")])]),v._v(\" \"),_(\"p\",[v._v(\"所以：\"),_(\"code\",[v._v(\"P(sun) = 0.75, P(rain) = 0.25\")])]),v._v(\" \"),_(\"p\",[v._v(\"这就是极限状态下的固定分布\")]),v._v(\" \"),_(\"p\",[_(\"strong\",[v._v(\"应用\")])]),v._v(\" \"),_(\"p\",[v._v(\"谷歌：搜索引擎，页面排名的想法\")]),v._v(\" \"),_(\"ol\",[_(\"li\",[v._v(\"手动排名，费时费力\")]),v._v(\" \"),_(\"li\",[v._v(\"当前页面是当前状态，从当前页面的链接跳转至下一状态（页面），正符合一个马尔可夫链，以此为基础排名\")])]),v._v(\" \"),_(\"p\",[v._v(\"由此构建一个马尔可夫模型，随着点击越来越多，排名将越来越精确\")]),v._v(\" \"),_(\"h4\",{attrs:{id:\"gibbs-sampling\"}},[_(\"a\",{staticClass:\"header-anchor\",attrs:{href:\"#gibbs-sampling\"}},[v._v(\"#\")]),v._v(\" Gibbs Sampling\")]),v._v(\" \"),_(\"blockquote\",[_(\"p\",[v._v(\"最常使用的抽样方法之一\")])]),v._v(\" \"),_(\"p\",[v._v(\"吉布斯抽样：先确定一组变量值，固定部分条件，选择单个变量进行抽样，重复如此，最终将汇聚到一个固定的概率\")]),v._v(\" \"),_(\"p\",[v._v(\"从抽样中改造马尔可夫模型\")]),v._v(\" \"),_(\"h3\",{attrs:{id:\"hidden-markov-models\"}},[_(\"a\",{staticClass:\"header-anchor\",attrs:{href:\"#hidden-markov-models\"}},[v._v(\"#\")]),v._v(\" Hidden Markov Models\")]),v._v(\" \"),_(\"p\",[v._v(\"天气HHM：一群住在地下室的学生通过观察教授是否带伞来推测今日外面是否下雨\")]),v._v(\" \"),_(\"table\",[_(\"thead\",[_(\"tr\",[_(\"th\",[v._v(\"R0（第一天）\")]),v._v(\" \"),_(\"th\",[v._v(\"R1（第二天）\")]),v._v(\" \"),_(\"th\",[v._v(\"P(R1|R0)\")])])]),v._v(\" \"),_(\"tbody\",[_(\"tr\",[_(\"td\",[v._v(\"+r\")]),v._v(\" \"),_(\"td\",[v._v(\"+r\")]),v._v(\" \"),_(\"td\",[v._v(\"0.7\")])]),v._v(\" \"),_(\"tr\",[_(\"td\",[v._v(\"+r\")]),v._v(\" \"),_(\"td\",[v._v(\"-r\")]),v._v(\" \"),_(\"td\",[v._v(\"0.3\")])]),v._v(\" \"),_(\"tr\",[_(\"td\",[v._v(\"-r\")]),v._v(\" \"),_(\"td\",[v._v(\"+r\")]),v._v(\" \"),_(\"td\",[v._v(\"0.3\")])]),v._v(\" \"),_(\"tr\",[_(\"td\",[v._v(\"-r\")]),v._v(\" \"),_(\"td\",[v._v(\"-r\")]),v._v(\" \"),_(\"td\",[v._v(\"0.7\")])])])]),v._v(\" \"),_(\"table\",[_(\"thead\",[_(\"tr\",[_(\"th\",[v._v(\"R（是否下雨）\")]),v._v(\" \"),_(\"th\",[v._v(\"U（是否带伞）\")]),v._v(\" \"),_(\"th\",[v._v(\"P(U|R)\")])])]),v._v(\" \"),_(\"tbody\",[_(\"tr\",[_(\"td\",[v._v(\"+r\")]),v._v(\" \"),_(\"td\",[v._v(\"+u\")]),v._v(\" \"),_(\"td\",[v._v(\"0.9\")])]),v._v(\" \"),_(\"tr\",[_(\"td\",[v._v(\"+r\")]),v._v(\" \"),_(\"td\",[v._v(\"-u\")]),v._v(\" \"),_(\"td\",[v._v(\"0.1\")])]),v._v(\" \"),_(\"tr\",[_(\"td\",[v._v(\"-r\")]),v._v(\" \"),_(\"td\",[v._v(\"+u\")]),v._v(\" \"),_(\"td\",[v._v(\"0.2\")])]),v._v(\" \"),_(\"tr\",[_(\"td\",[v._v(\"-r\")]),v._v(\" \"),_(\"td\",[v._v(\"-u\")]),v._v(\" \"),_(\"td\",[v._v(\"0.8\")])])])]),v._v(\" \"),_(\"p\",[v._v(\"我们不能仅仅因为教授带伞便推断下雨，因素是多方面的且不确定的\")]),v._v(\" \"),_(\"p\",[v._v(\"证据变量之间并不保证独立，D分离后的三元组显示其有关联\")]),v._v(\" \"),_(\"p\",[v._v(\"一个简单的隐式马尔科夫模型的贝叶斯网\")]),v._v(\" \"),_(\"div\",{staticClass:\"language- line-numbers-mode\"},[_(\"pre\",{pre:!0,attrs:{class:\"language-text\"}},[_(\"code\",[v._v(\"X1———X2———X3———X4\\n|\\t |\\t  |\\t   |\\nE1\\t E2\\t  E3   E4\\n\")])]),v._v(\" \"),_(\"div\",{staticClass:\"line-numbers-wrapper\"},[_(\"span\",{staticClass:\"line-number\"},[v._v(\"1\")]),_(\"br\"),_(\"span\",{staticClass:\"line-number\"},[v._v(\"2\")]),_(\"br\"),_(\"span\",{staticClass:\"line-number\"},[v._v(\"3\")]),_(\"br\")])]),_(\"p\",[_(\"strong\",[v._v(\"examples\")])]),v._v(\" \"),_(\"p\",[v._v(\"语音识别\")]),v._v(\" \"),_(\"ul\",[_(\"li\",[v._v(\"Observations：声波\")]),v._v(\" \"),_(\"li\",[v._v(\"States：字符\")])]),v._v(\" \"),_(\"p\",[v._v(\"机器翻译\")]),v._v(\" \"),_(\"ul\",[_(\"li\",[v._v(\"Observations：字符\")]),v._v(\" \"),_(\"li\",[v._v(\"States：最佳翻译\")])]),v._v(\" \"),_(\"p\",[v._v(\"机器人跟踪\")]),v._v(\" \"),_(\"ul\",[_(\"li\",[v._v(\"Observations：激光测距读数\")]),v._v(\" \"),_(\"li\",[v._v(\"States：位置数据\")])]),v._v(\" \"),_(\"p\",[_(\"strong\",[v._v(\"Inference\")])]),v._v(\" \"),_(\"p\",[_(\"code\",[v._v(\"P(X1|e1) = P(x1,e1) / P(e1) = αP(x1|e1)\")])]),v._v(\" \"),_(\"p\",[v._v(\"使其规范化（概率和化为1），这样就摆脱了证据变量e1\\n$$\\n\\\\begin{align}\\nP(X_{t+1}|e_t) &= \\\\sum P(X_{t+1},x_t|e_t)\\\\&=\\\\sum P(X_{t+1}|x_t,e_t)P(x_t|e_t)\\\\&=\\\\sum\\nP(X_{t+1}|x_t)P(x_t|e_t)\\n\\\\end{align}\\n$$\\n吃豆人的隐式马尔可夫模型：只知道鬼魂和自己的距离，推断鬼魂的位置并吃掉它\")]),v._v(\" \"),_(\"p\",[v._v(\"Haven't figure out\")]),v._v(\" \"),_(\"ul\",[_(\"li\",[v._v(\"Belief Update\")]),v._v(\" \"),_(\"li\",[v._v(\"Observation\")]),v._v(\" \"),_(\"li\",[v._v(\"Online Belief Update\")]),v._v(\" \"),_(\"li\",[v._v(\"The Forward Algorithm\")])]),v._v(\" \"),_(\"p\",[v._v(\"在隐马尔可夫模型中，证据驱动着结果推进\")]),v._v(\" \"),_(\"p\",[v._v(\"当没有证据时，就是一个普通的马尔可夫模型，各状态概率随时间推移将收敛至定值\")]),v._v(\" \"),_(\"h3\",{attrs:{id:\"particle-filtering\"}},[_(\"a\",{staticClass:\"header-anchor\",attrs:{href:\"#particle-filtering\"}},[v._v(\"#\")]),v._v(\" Particle Filtering\")]),v._v(\" \"),_(\"blockquote\",[_(\"p\",[v._v(\"粒子滤波\")])]),v._v(\" \"),_(\"h4\",{attrs:{id:\"对于单独的变量x\"}},[_(\"a\",{staticClass:\"header-anchor\",attrs:{href:\"#对于单独的变量x\"}},[v._v(\"#\")]),v._v(\" 对于单独的变量X\")]),v._v(\" \"),_(\"p\",[_(\"code\",[v._v(\"P(x2) = Σ(x1)P(x1,x2) = Σ(x1)P(x1)P(x2|x1)\")])]),v._v(\" \"),_(\"p\",[v._v(\"对于x1的所有取值，x2发生的概率之和，我们对所有x2的取值做这一操作，那么就得到x2的概率分布\")]),v._v(\" \"),_(\"p\",[v._v(\"现在对变量X加上证据E\")]),v._v(\" \"),_(\"p\",[_(\"code\",[v._v(\"B(Xt) = P(Xt|e(1:t))\")])]),v._v(\" \"),_(\"ul\",[_(\"li\",[v._v(\"B指对变量\"),_(\"code\",[v._v(\"Xt\")]),v._v(\"的信念（Belief），其中\"),_(\"code\",[v._v(\"t\")]),v._v(\"为时间坐标，即第几个\"),_(\"code\",[v._v(\"X\")])]),v._v(\" \"),_(\"li\",[v._v(\"称为信念分布\")]),v._v(\" \"),_(\"li\",[v._v(\"它表示变量\"),_(\"code\",[v._v(\"Xt\")]),v._v(\"的概率确实与证据\"),_(\"code\",[v._v(\"e(1:t)\")]),v._v(\"有关\")])]),v._v(\" \"),_(\"p\",[v._v(\"我们想知道的是：\"),_(\"code\",[v._v(\"P(X(t+1)|e(1:t))\")])]),v._v(\" \"),_(\"ul\",[_(\"li\",[v._v(\"通过上一步的证据推断现在的概率\")])]),v._v(\" \"),_(\"p\",[v._v(\"公式变换：\")]),v._v(\" \"),_(\"p\",[_(\"code\",[v._v(\"P(X(t+1)|e(1:t)) = ΣP(X(t+1),xt|e(1:t))\")]),v._v(\"因为xt是已经发生的事，所以加在左边不影响概率\")]),v._v(\" \"),_(\"p\",[v._v(\"​                             \"),_(\"code\",[v._v(\"= ΣP(X(t+1)|xt,e(1:t)) P(xt|e(1:t))\")]),v._v(\"条件概率分解\")]),v._v(\" \"),_(\"p\",[v._v(\"​\\t\\t\\t\\t\\t\\t\\t\"),_(\"code\",[v._v(\"= ΣP(X(t+1)|xt) P(xt|e(1:t))\")]),v._v(\"当\"),_(\"code\",[v._v(\"xt\")]),v._v(\"发生时，\"),_(\"code\",[v._v(\"X(t+1)\")]),v._v(\"和\"),_(\"code\",[v._v(\"e(1:t)\")]),v._v(\"独立\")]),v._v(\" \"),_(\"p\",[v._v(\"概率转移房产：\"),_(\"code\",[v._v(\"P(X(t+1)|e(1:t)) = ΣP(X(t+1)|xt) P(xt|e(1:t))\")])]),v._v(\" \"),_(\"p\",[v._v(\"信念转移方程：\"),_(\"code\",[v._v(\"B(X(t+1)) = ΣP(X(t+1)|xt) B(xt)\")]),v._v(\"（\"),_(\"code\",[v._v(\"B(X(t+1)) = P(X(t+1)|e(1:t+1))\")]),v._v(\"）\")]),v._v(\" \"),_(\"h4\",{attrs:{id:\"对于有证据的变量x-e\"}},[_(\"a\",{staticClass:\"header-anchor\",attrs:{href:\"#对于有证据的变量x-e\"}},[v._v(\"#\")]),v._v(\" 对于有证据的变量X|E\")]),v._v(\" \"),_(\"p\",[v._v(\"已知\"),_(\"code\",[v._v(\"P(x1)\")])]),v._v(\" \"),_(\"p\",[_(\"code\",[v._v(\"P(x1|e1) = P(x1,e1) / P(e1)\")])]),v._v(\" \"),_(\"p\",[v._v(\"​\\t\\t\\t\\t\"),_(\"code\",[v._v(\"α P(x1,e1)\")]),v._v(\"将P(e1)看做常数\")]),v._v(\" \"),_(\"p\",[v._v(\"​\\t\\t\\t\\t\"),_(\"code\",[v._v(\"= P(x1)P(e1|x1)\")]),v._v(\"转换x1为条件\")]),v._v(\" \"),_(\"p\",[v._v(\"为什么这么做，对于每一个x1都存在一个P(e1)，求出所有的P(e1)，对他们进行规范化，得到一张证据概率分布表\")]),v._v(\" \"),_(\"p\",[_(\"strong\",[v._v(\"Observation\")])]),v._v(\" \"),_(\"p\",[v._v(\"如何工作？\")]),v._v(\" \"),_(\"p\",[v._v(\"假设我们知道\"),_(\"code\",[v._v(\"B(X(t+1)) = P(X(t+1)|e(1:t))\")])]),v._v(\" \"),_(\"p\",[v._v(\"当新的证据进入\")]),v._v(\" \"),_(\"p\",[_(\"code\",[v._v(\"P(X(t+1)|e(1:t+1)) = P(X(t+1), e(t+1)|e(1:t)) / P(e(t+1)|e(1:t))\")])]),v._v(\" \"),_(\"p\",[v._v(\"​\\t\\t\\t\\t\\t\\t\\t\\t\"),_(\"code\",[v._v(\"α P(X(t+1), e(t+1)|e(1:t))\")])]),v._v(\" \"),_(\"p\",[v._v(\"​\\t\\t\\t\\t\\t\\t\\t\\t\"),_(\"code\",[v._v(\"= P(X(t+1)|e(1:t)) P(e(t+1)|X(t+1),e(1:t))\")])]),v._v(\" \"),_(\"p\",[v._v(\"​\\t\\t\\t\\t\\t\\t\\t\\t\"),_(\"code\",[v._v(\"= P(X(t+1)|e(1:t)) P(e(t+1)|X(t+1))\")]),v._v(\"当前\"),_(\"code\",[v._v(\"e(1:t)\")]),v._v(\"已经是事实\")]),v._v(\" \"),_(\"p\",[v._v(\"得到：\"),_(\"code\",[v._v(\"B(X(t+1)) α P(e(t+1)|X(t+1)) B(X(t+1))\")])]),v._v(\" \"),_(\"p\",[v._v(\"新的信念分布是由旧的信念分布根据所有证据加权处理并重新规范化得出的\")]),v._v(\" \"),_(\"p\",[v._v(\"总结上面两个例子：\")]),v._v(\" \"),_(\"ul\",[_(\"li\",[_(\"p\",[_(\"code\",[v._v(\"P(Xt|e(1:t-1)) = ΣP(X(t-1)|e(1:t-1)) P(xt|x(t-1))\")])])]),v._v(\" \"),_(\"li\",[_(\"p\",[_(\"code\",[v._v(\"P(Xt|e(1:t)) α P(X(t)|e(1:t-1)) P(et|xt))\")])]),v._v(\" \"),_(\"p\",[v._v(\"α 指正比例，即需要规范化为1\")])])]),v._v(\" \"),_(\"h4\",{attrs:{id:\"filtering\"}},[_(\"a\",{staticClass:\"header-anchor\",attrs:{href:\"#filtering\"}},[v._v(\"#\")]),v._v(\" Filtering\")]),v._v(\" \"),_(\"blockquote\",[_(\"p\",[_(\"code\",[v._v(\"xt\")]),v._v(\"是隐藏变量，\"),_(\"code\",[v._v(\"et\")]),v._v(\"是当前证据\")]),v._v(\" \"),_(\"p\",[v._v(\"Filtering：approximate solution\")]),v._v(\" \"),_(\"p\",[v._v(\"Particle：粒子，样本\")])]),v._v(\" \"),_(\"p\",[v._v(\"有时|X|过大无法准确推断，我们只能使用近似推断：\")]),v._v(\" \"),_(\"ul\",[_(\"li\",[v._v(\"追踪X的样本\")]),v._v(\" \"),_(\"li\",[v._v(\"样本被称为粒子\")]),v._v(\" \"),_(\"li\",[v._v(\"花费的时间是粒子数量的线性\")]),v._v(\" \"),_(\"li\",[v._v(\"样本数量需要够大\")]),v._v(\" \"),_(\"li\",[v._v(\"储存的是样本列表，而不是所有可能性的总表\")])]),v._v(\" \"),_(\"p\",[v._v(\"对一堆粒子，带着证据对他们进行测试，观察\"),_(\"code\",[v._v(\"P(xi|e)\")]),v._v(\"大小，并进行加权\")]),v._v(\" \"),_(\"p\",[v._v(\"当某一粒子权重过小（不符合证据），将新增粒子将其代替（未加权的粒子），再重新根据证据进行加权\")]),v._v(\" \"),_(\"p\",[v._v(\"注意我们始终关心的是那个粒子列表，而不是概率分布表\")]),v._v(\" \"),_(\"p\",[v._v(\"当得到新的证据时，将重新进行加权，根据特定的证据，权重总会集中在一起，当没有证据加入，粒子将发生分歧逐渐分散\")]),v._v(\" \"),_(\"p\",[v._v(\"我的理解就是：\")]),v._v(\" \"),_(\"p\",[v._v(\"有一大堆测试粒子，你不断给出证据去推翻他们，在幸存的粒子中找到答案\")]),v._v(\" \"),_(\"h3\",{attrs:{id:\"applications-of-hmms\"}},[_(\"a\",{staticClass:\"header-anchor\",attrs:{href:\"#applications-of-hmms\"}},[v._v(\"#\")]),v._v(\" Applications of HMMs\")]),v._v(\" \"),_(\"blockquote\",[_(\"p\",[v._v(\"隐式马尔科夫应用\")])]),v._v(\" \"),_(\"h4\",{attrs:{id:\"robot-localization\"}},[_(\"a\",{staticClass:\"header-anchor\",attrs:{href:\"#robot-localization\"}},[v._v(\"#\")]),v._v(\" Robot localization\")]),v._v(\" \"),_(\"p\",[v._v(\"确定自己在地图中的方位\")]),v._v(\" \"),_(\"ul\",[_(\"li\",[v._v(\"知道地图，但不知道位置\")]),v._v(\" \"),_(\"li\",[v._v(\"观察值可能是一系列距边缘距离\")]),v._v(\" \"),_(\"li\",[v._v(\"读数通常是连续的\")]),v._v(\" \"),_(\"li\",[v._v(\"粒子过滤是主要解题手段\")])]),v._v(\" \"),_(\"p\",[v._v(\"一开始粒子分布在整个地图中，随着机器人不断发现证据，如右边是一堵墙，那么弱化所有右边没有墙的粒子，直到只剩一坨或一个粒子\")]),v._v(\" \"),_(\"h4\",{attrs:{id:\"robat-mapping\"}},[_(\"a\",{staticClass:\"header-anchor\",attrs:{href:\"#robat-mapping\"}},[v._v(\"#\")]),v._v(\" Robat Mapping\")]),v._v(\" \"),_(\"p\",[v._v(\"画图机器人\")]),v._v(\" \"),_(\"ul\",[_(\"li\",[v._v(\"不知道地图和自身方位\")]),v._v(\" \"),_(\"li\",[v._v(\"状态由位置和地图构成\")]),v._v(\" \"),_(\"li\",[v._v(\"主要使用技术：Kalman filtering（Gaussian HMMs）and paricle methods\")])]),v._v(\" \"),_(\"p\",[v._v(\"SLAM：simultanrous localization and mapping（同步本地化和映射）\")]),v._v(\" \"),_(\"h4\",{attrs:{id:\"dynamic-bayes-nets\"}},[_(\"a\",{staticClass:\"header-anchor\",attrs:{href:\"#dynamic-bayes-nets\"}},[v._v(\"#\")]),v._v(\" Dynamic Bayes Nets\")]),v._v(\" \"),_(\"p\",[v._v(\"动态贝叶斯网络\")]),v._v(\" \"),_(\"p\",[v._v(\"同时存在多个变量，我们将变量分开分成不同的HMM问题，分别进行粒子过滤\")]),v._v(\" \"),_(\"h2\",{attrs:{id:\"ml\"}},[_(\"a\",{staticClass:\"header-anchor\",attrs:{href:\"#ml\"}},[v._v(\"#\")]),v._v(\" ML\")]),v._v(\" \"),_(\"blockquote\",[_(\"p\",[v._v(\"机器学习：machine learning\")]),v._v(\" \"),_(\"p\",[v._v(\"神经网络\")])]),v._v(\" \"),_(\"p\",[v._v(\"在之前的例子中，世界的规则都是给定的：游戏规则，条件概率分布等等\")]),v._v(\" \"),_(\"p\",[v._v(\"当我们没有给定的模型时，如何去建立一个相对精确的AI代理\")]),v._v(\" \"),_(\"p\",[v._v(\"从数据中获取模型，以希望建立准确的系统\")]),v._v(\" \"),_(\"h3\",{attrs:{id:\"naive-bayes\"}},[_(\"a\",{staticClass:\"header-anchor\",attrs:{href:\"#naive-bayes\"}},[v._v(\"#\")]),v._v(\" Naive Bayes\")]),v._v(\" \"),_(\"blockquote\",[_(\"p\",[v._v(\"朴素贝叶斯\")]),v._v(\" \"),_(\"p\",[v._v(\"基于模型的分类\")])]),v._v(\" \"),_(\"h4\",{attrs:{id:\"classification\"}},[_(\"a\",{staticClass:\"header-anchor\",attrs:{href:\"#classification\"}},[v._v(\"#\")]),v._v(\" Classification\")]),v._v(\" \"),_(\"blockquote\",[_(\"p\",[v._v(\"分类\")]),v._v(\" \"),_(\"p\",[v._v(\"分类不是唯一的机器学习，但可能是最大的一类\")])]),v._v(\" \"),_(\"p\",[_(\"strong\",[v._v(\"Examples\")])]),v._v(\" \"),_(\"p\",[v._v(\"1、Spam Filter\")]),v._v(\" \"),_(\"p\",[v._v(\"垃圾邮件过滤\")]),v._v(\" \"),_(\"p\",[v._v(\"通常来说，获得正确的数据是构建和部署中最难的部分，比如在学习过滤邮件的过程中，需要手动标注垃圾邮件让机器进行学习；部署机器学习系统的难度取决于收集数据和发现数据是否自然\")]),v._v(\" \"),_(\"p\",[v._v(\"垃圾邮件检测不一定来自于内容或单个词语，还需要考虑发件人、发送环境等一系列问题\")]),v._v(\" \"),_(\"p\",[v._v(\"2、Digit Recognition\")]),v._v(\" \"),_(\"p\",[v._v(\"识别手写阿拉伯数字\")]),v._v(\" \"),_(\"p\",[v._v(\"检查一个像素网络，需要收集大量贴上标签的实例图像\")]),v._v(\" \"),_(\"ul\",[_(\"li\",[v._v(\"医疗诊断\")]),v._v(\" \"),_(\"li\",[v._v(\"投资风险诊断\")]),v._v(\" \"),_(\"li\",[v._v(\"自动论文评分\")]),v._v(\" \"),_(\"li\",[v._v(\"自然语言处理\")])]),v._v(\" \"),_(\"h4\",{attrs:{id:\"model-based-classification\"}},[_(\"a\",{staticClass:\"header-anchor\",attrs:{href:\"#model-based-classification\"}},[v._v(\"#\")]),v._v(\" Model-Based Classification\")]),v._v(\" \"),_(\"p\",[v._v(\"建立一个模型，一个最简单的贝叶斯网络：朴素贝叶斯网络\")]),v._v(\" \"),_(\"p\",[v._v(\"一对多的贝叶斯网，单个类y对应多个功能f(i)\")]),v._v(\" \"),_(\"ul\",[_(\"li\",[v._v(\"一个非常激进的假设\")])]),v._v(\" \"),_(\"p\",[v._v(\"在分类模型中Y对应的是类，而Fi对应类的诸多功能或特点\")]),v._v(\" \"),_(\"p\",[v._v(\"在朴素贝叶斯网络中，我们要计算的其实是概率向量\")]),v._v(\" \"),_(\"p\",[_(\"code\",[v._v(\"P(Y,f1...fn) = [P(y1,f1...fn), P(y2,f1...fn)...](T)\")])]),v._v(\" \"),_(\"p\",[v._v(\"即各种特征对应类的条件概率大小，根据条件概率可得公式：\"),_(\"code\",[v._v(\"P(y1,f1...fn) = P(y1)ΠP(fi|y1)\")]),v._v(\"（变量消除）\")]),v._v(\" \"),_(\"p\",[v._v(\"f是特征，y是类，上式表示类y1和特征fi们同时出现的概率大小，这取决于特征的条件概率大小和类的占比\")]),v._v(\" \"),_(\"p\",[v._v(\"各个特征之间的概率是独立的\")]),v._v(\" \"),_(\"p\",[v._v(\"如垃圾邮件过滤，W表示words，ham表示有用的邮件，spam表示垃圾邮件\")]),v._v(\" \"),_(\"p\",[v._v(\"对于\"),_(\"code\",[v._v(\"P(W|spam)\")])]),v._v(\" \"),_(\"ul\",[_(\"li\",[v._v(\"the: 0.0156\")]),v._v(\" \"),_(\"li\",[v._v(\"to: 0.0153\")]),v._v(\" \"),_(\"li\",[v._v(\"and: 0.0115\")]),v._v(\" \"),_(\"li\",[v._v(\"of: 0.0095\")]),v._v(\" \"),_(\"li\",[v._v(\"you: 0.0093\")]),v._v(\" \"),_(\"li\",[v._v(\"a: 0.0086\")]),v._v(\" \"),_(\"li\",[v._v(\"with: 0.008\")]),v._v(\" \"),_(\"li\",[v._v(\"from: 0.0075\")])]),v._v(\" \"),_(\"p\",[v._v(\"而对于\"),_(\"code\",[v._v(\"P(W|ham)\")])]),v._v(\" \"),_(\"ul\",[_(\"li\",[v._v(\"the: 0.021\")]),v._v(\" \"),_(\"li\",[v._v(\"to: 0.133\")]),v._v(\" \"),_(\"li\",[v._v(\"of: 0.119\")]),v._v(\" \"),_(\"li\",[v._v(\"2002: 0.011\")]),v._v(\" \"),_(\"li\",[v._v(\"from: 0.107\")]),v._v(\" \"),_(\"li\",[v._v(\"and: 0.105\")]),v._v(\" \"),_(\"li\",[v._v(\"a: 0.1\")])]),v._v(\" \"),_(\"p\",[v._v(\"相同的单词，给予不同的条件，其出现的概率不同\")]),v._v(\" \"),_(\"p\",[v._v(\"每当有新的单词\"),_(\"code\",[v._v(\"wi\")]),v._v(\"录入时，原先的\"),_(\"code\",[v._v(\"P(W|ham)/P(W|spam)\")]),v._v(\"都将乘上这个新\"),_(\"code\",[v._v(\"P(wi|ham)/P(wi|spam)\")]),v._v(\"，更新分类概率\")]),v._v(\" \"),_(\"p\",[v._v(\"对于求得的\"),_(\"code\",[v._v(\"P(W|Y)\")]),v._v(\"重新规范化，在\"),_(\"code\",[v._v(\"P(W|Y)\")]),v._v(\"中选取概率最大的\"),_(\"code\",[v._v(\"P(W|y)\")]),v._v(\"，作为当前邮件的分类\"),_(\"code\",[v._v(\"y\")])]),v._v(\" \"),_(\"h4\",{attrs:{id:\"training-testing\"}},[_(\"a\",{staticClass:\"header-anchor\",attrs:{href:\"#training-testing\"}},[v._v(\"#\")]),v._v(\" Training & Testing\")]),v._v(\" \"),_(\"p\",[v._v(\"我们应该从数据中构建朴素贝叶斯网络，只有有了相对准确的模型才能进行推理或分类\")]),v._v(\" \"),_(\"p\",[v._v(\"How to make progress?\")]),v._v(\" \"),_(\"p\",[v._v(\"Training and generalize（训练和概括）\")]),v._v(\" \"),_(\"p\",[v._v(\"训练代表了概率分布，数据越多，概率将越精确，数据不够时，将不能进行合适的模拟；如果使用一种不能概括的方式来学习，将无法对数据进行合理的分配，也就不能处理多变的情况\")]),v._v(\" \"),_(\"p\",[v._v(\"也不能过度追求精确而导致过度拟合，失去一般性\")]),v._v(\" \"),_(\"p\",[v._v(\"一些重要的组成部分：\")]),v._v(\" \"),_(\"p\",[v._v(\"Data：带标签的实例\")]),v._v(\" \"),_(\"ul\",[_(\"li\",[v._v(\"Training set\")]),v._v(\" \"),_(\"li\",[v._v(\"Held out set\")]),v._v(\" \"),_(\"li\",[v._v(\"Test set\")])]),v._v(\" \"),_(\"p\",[v._v(\"Features：特征属性\")]),v._v(\" \"),_(\"p\",[v._v(\"Experimentation cycle：循环测试\")]),v._v(\" \"),_(\"ul\",[_(\"li\",[v._v(\"Learn parameters on training set\")]),v._v(\" \"),_(\"li\",[v._v(\"Compute accuracy of test set\")]),v._v(\" \"),_(\"li\",[v._v('Vary important: never \"peek\" at the test set')])]),v._v(\" \"),_(\"p\",[v._v(\"Evaluation：精确拟合\")]),v._v(\" \"),_(\"ul\",[_(\"li\",[v._v(\"Accuracy: fraction of instances predicted correctly\")])]),v._v(\" \"),_(\"p\",[v._v(\"Overfitting and generalization：过度拟合和概括\")]),v._v(\" \"),_(\"p\",[v._v(\"我们真正想要的是实用程序，而不仅仅是精确性，这意味着能够适用于各种不同情况\")]),v._v(\" \"),_(\"p\",[v._v(\"垃圾邮件是一个糟糕的例子，因为垃圾邮件是人为制造的，这里存在了一个军备竞赛，他是对抗性的\")]),v._v(\" \"),_(\"h4\",{attrs:{id:\"generalization-overfitting\"}},[_(\"a\",{staticClass:\"header-anchor\",attrs:{href:\"#generalization-overfitting\"}},[v._v(\"#\")]),v._v(\" Generalization & Overfitting\")]),v._v(\" \"),_(\"p\",[v._v(\"我们希望程序适应恒定的功能而不是某一个特定的情况\")]),v._v(\" \"),_(\"p\",[v._v(\"UnderFitting：不合适\")]),v._v(\" \"),_(\"ul\",[_(\"li\",[v._v(\"用下降曲线模拟上升曲线\")])]),v._v(\" \"),_(\"p\",[v._v(\"OverFitting：过度拟合\")]),v._v(\" \"),_(\"ul\",[_(\"li\",[v._v(\"如用一个15项多项式函数去细致模拟一堆看起来像二次函数的点\")]),v._v(\" \"),_(\"li\",[v._v(\"又如当某一个条件概率过度拟合为0，即使分类C1在其余概率上远大于C2，但因为这个特征概率为0，总概率变为0，这是不合理的\")])]),v._v(\" \"),_(\"p\",[v._v(\"将概率设为0是很危险的行为\")]),v._v(\" \"),_(\"h4\",{attrs:{id:\"parameter-estimation\"}},[_(\"a\",{staticClass:\"header-anchor\",attrs:{href:\"#parameter-estimation\"}},[v._v(\"#\")]),v._v(\" Parameter Estimation\")]),v._v(\" \"),_(\"blockquote\",[_(\"p\",[v._v(\"参数估计\")])]),v._v(\" \"),_(\"p\",[v._v(\"use training data to learning:：\"),_(\"code\",[v._v(\"P(ML)(x) = count(x) / total samples\")])]),v._v(\" \"),_(\"p\",[v._v(\"在这里我们选择最大化数据估计可能性\")]),v._v(\" \"),_(\"ul\",[_(\"li\",[v._v(\"maximizes the likelihood of the data\")])]),v._v(\" \"),_(\"p\",[_(\"strong\",[v._v(\"Smoothing\")])]),v._v(\" \"),_(\"p\",[v._v(\"对于最大可能性估计：\"),_(\"code\",[v._v(\"P(ML)(x) = count(x) / total samples\")])]),v._v(\" \"),_(\"p\",[v._v(\"给予一些缓冲\")]),v._v(\" \"),_(\"p\",[_(\"code\",[v._v(\"P = arg maxP(α|X) = arg maxP(X|α) P(α) P(X) = arg maxP(X|α) P(α)\")])]),v._v(\" \"),_(\"p\",[_(\"strong\",[v._v(\"Laplace Somoothing\")])]),v._v(\" \"),_(\"p\",[v._v(\"Unseen Events\")]),v._v(\" \"),_(\"p\",[v._v(\"考虑没有观察到的事物\")]),v._v(\" \"),_(\"p\",[v._v(\"如对于三个球：red、red、blue\")]),v._v(\" \"),_(\"p\",[v._v(\"最大概率估计\")]),v._v(\" \"),_(\"ul\",[_(\"li\",[_(\"code\",[v._v(\"P(ML)(X) = [2/3, 1/3]\")])])]),v._v(\" \"),_(\"p\",[v._v(\"而拉普拉斯扩展方法将加上一个隐藏的红球和一个隐藏的蓝球，是概率更加平滑\")]),v._v(\" \"),_(\"ul\",[_(\"li\",[_(\"code\",[v._v(\"P(LAP,1)(X) = [3/5, 2/5]\")])])]),v._v(\" \"),_(\"p\",[v._v(\"我们也可以假设有100个隐藏的红球和蓝球\")]),v._v(\" \"),_(\"ul\",[_(\"li\",[_(\"code\",[v._v(\"P(LAP,100)(X) = [102/203, 101/203]\")])])]),v._v(\" \"),_(\"p\",[v._v(\"使用更加平滑的概率来控制过度拟合\")]),v._v(\" \"),_(\"p\",[v._v(\"Tuning：调整\")]),v._v(\" \"),_(\"p\",[v._v(\"从训练数据中获取参数\")]),v._v(\" \"),_(\"p\",[v._v(\"Features：特征\")]),v._v(\" \"),_(\"p\",[v._v(\"Errors：模型发生错误时该怎么做\")]),v._v(\" \"),_(\"ul\",[_(\"li\",[v._v(\"需要更多特征和训练数据\")]),v._v(\" \"),_(\"li\",[v._v(\"在朴素贝叶斯网络中增加变量\")])]),v._v(\" \"),_(\"h3\",{attrs:{id:\"perceptrons\"}},[_(\"a\",{staticClass:\"header-anchor\",attrs:{href:\"#perceptrons\"}},[v._v(\"#\")]),v._v(\" Perceptrons\")]),v._v(\" \"),_(\"blockquote\",[_(\"p\",[v._v(\"感知器\")])]),v._v(\" \"),_(\"h3\",{attrs:{id:\"logistic-regression\"}},[_(\"a\",{staticClass:\"header-anchor\",attrs:{href:\"#logistic-regression\"}},[v._v(\"#\")]),v._v(\" Logistic Regression\")]),v._v(\" \"),_(\"blockquote\",[_(\"p\",[v._v(\"逻辑回归\")])]),v._v(\" \"),_(\"h2\",{attrs:{id:\"supplement\"}},[_(\"a\",{staticClass:\"header-anchor\",attrs:{href:\"#supplement\"}},[v._v(\"#\")]),v._v(\" Supplement\")]),v._v(\" \"),_(\"blockquote\",[_(\"p\",[v._v(\"一些关于算法和模型的补充\")])]),v._v(\" \"),_(\"h3\",{attrs:{id:\"beam-search\"}},[_(\"a\",{staticClass:\"header-anchor\",attrs:{href:\"#beam-search\"}},[v._v(\"#\")]),v._v(\" Beam Search\")]),v._v(\" \"),_(\"blockquote\",[_(\"p\",[v._v(\"束搜索，属于局部搜索\")])]),v._v(\" \"),_(\"p\",[_(\"a\",{attrs:{href:\"https://zhuanlan.zhihu.com/p/82829880\",target:\"_blank\",rel:\"noopener noreferrer\"}},[v._v(\"如何通俗的理解beam search？ 知乎\"),_(\"OutboundLink\")],1)]),v._v(\" \"),_(\"p\",[v._v(\"假设现在有一个简易的翻译任务，将中文翻译成英文简写，如\")]),v._v(\" \"),_(\"ul\",[_(\"li\",[v._v(\"我恨你 ——> i hate you ——> i h u\")]),v._v(\" \"),_(\"li\",[v._v(\"我爱你 ——> i love you ——> i l u\")])]),v._v(\" \"),_(\"p\",[v._v(\"为了简化问题，现在我们的英文字典里只有三个字母，如\")]),v._v(\" \"),_(\"ul\",[_(\"li\",[v._v(\"I\")]),v._v(\" \"),_(\"li\",[v._v(\"H\")]),v._v(\" \"),_(\"li\",[v._v(\"U\")])]),v._v(\" \"),_(\"p\",[v._v(\"现在我们要把“我恨你”通过字典翻译成最佳的英文简写\")]),v._v(\" \"),_(\"h4\",{attrs:{id:\"exhaustive-search\"}},[_(\"a\",{staticClass:\"header-anchor\",attrs:{href:\"#exhaustive-search\"}},[v._v(\"#\")]),v._v(\" Exhaustive Search\")]),v._v(\" \"),_(\"blockquote\",[_(\"p\",[v._v(\"穷举搜索\")])]),v._v(\" \"),_(\"p\",[v._v(\"穷举所有的排列可能性：\"),_(\"code\",[v._v(\"A(3,3) = 3*3*3 = 27\")])]),v._v(\" \"),_(\"div\",{staticClass:\"language- line-numbers-mode\"},[_(\"pre\",{pre:!0,attrs:{class:\"language-text\"}},[_(\"code\",[v._v(\"I-I-I\\nI-I-H\\nI-I-U\\nI-H-I\\nI-H-H\\nI-H-U\\nI-U-I\\nI-U-H\\nI-U-U\\n\\nH-I-I\\nH-I-H\\nH-I-U\\nH-H-I\\nH-H-H\\nH-H-U\\nH-U-I\\nH-U-H\\nH-U-U\\n\\nU-I-I\\nU-I-H\\nU-I-U\\nU-H-I\\nU-H-H\\nU-H-U\\nU-U-I\\nU-U-H\\nU-U-U\\n\")])]),v._v(\" \"),_(\"div\",{staticClass:\"line-numbers-wrapper\"},[_(\"span\",{staticClass:\"line-number\"},[v._v(\"1\")]),_(\"br\"),_(\"span\",{staticClass:\"line-number\"},[v._v(\"2\")]),_(\"br\"),_(\"span\",{staticClass:\"line-number\"},[v._v(\"3\")]),_(\"br\"),_(\"span\",{staticClass:\"line-number\"},[v._v(\"4\")]),_(\"br\"),_(\"span\",{staticClass:\"line-number\"},[v._v(\"5\")]),_(\"br\"),_(\"span\",{staticClass:\"line-number\"},[v._v(\"6\")]),_(\"br\"),_(\"span\",{staticClass:\"line-number\"},[v._v(\"7\")]),_(\"br\"),_(\"span\",{staticClass:\"line-number\"},[v._v(\"8\")]),_(\"br\"),_(\"span\",{staticClass:\"line-number\"},[v._v(\"9\")]),_(\"br\"),_(\"span\",{staticClass:\"line-number\"},[v._v(\"10\")]),_(\"br\"),_(\"span\",{staticClass:\"line-number\"},[v._v(\"11\")]),_(\"br\"),_(\"span\",{staticClass:\"line-number\"},[v._v(\"12\")]),_(\"br\"),_(\"span\",{staticClass:\"line-number\"},[v._v(\"13\")]),_(\"br\"),_(\"span\",{staticClass:\"line-number\"},[v._v(\"14\")]),_(\"br\"),_(\"span\",{staticClass:\"line-number\"},[v._v(\"15\")]),_(\"br\"),_(\"span\",{staticClass:\"line-number\"},[v._v(\"16\")]),_(\"br\"),_(\"span\",{staticClass:\"line-number\"},[v._v(\"17\")]),_(\"br\"),_(\"span\",{staticClass:\"line-number\"},[v._v(\"18\")]),_(\"br\"),_(\"span\",{staticClass:\"line-number\"},[v._v(\"19\")]),_(\"br\"),_(\"span\",{staticClass:\"line-number\"},[v._v(\"20\")]),_(\"br\"),_(\"span\",{staticClass:\"line-number\"},[v._v(\"21\")]),_(\"br\"),_(\"span\",{staticClass:\"line-number\"},[v._v(\"22\")]),_(\"br\"),_(\"span\",{staticClass:\"line-number\"},[v._v(\"23\")]),_(\"br\"),_(\"span\",{staticClass:\"line-number\"},[v._v(\"24\")]),_(\"br\"),_(\"span\",{staticClass:\"line-number\"},[v._v(\"25\")]),_(\"br\"),_(\"span\",{staticClass:\"line-number\"},[v._v(\"26\")]),_(\"br\"),_(\"span\",{staticClass:\"line-number\"},[v._v(\"27\")]),_(\"br\"),_(\"span\",{staticClass:\"line-number\"},[v._v(\"28\")]),_(\"br\"),_(\"span\",{staticClass:\"line-number\"},[v._v(\"29\")]),_(\"br\")])]),_(\"p\",[v._v(\"即在每次扩展树时，都完全扩展，完全扩展指每个节点都进行所有可能性的扩展\")]),v._v(\" \"),_(\"p\",[v._v(\"这样一定能找到全局最优解：\"),_(\"code\",[v._v(\"IHU\")])]),v._v(\" \"),_(\"h4\",{attrs:{id:\"greedy-search\"}},[_(\"a\",{staticClass:\"header-anchor\",attrs:{href:\"#greedy-search\"}},[v._v(\"#\")]),v._v(\" Greedy Search\")]),v._v(\" \"),_(\"blockquote\",[_(\"p\",[v._v(\"贪心搜索\")])]),v._v(\" \"),_(\"p\",[v._v(\"贪心算法有点像本地搜索：即在每步都选取当前最优解（条件概率最大的节点）进行扩展\")]),v._v(\" \"),_(\"p\",[v._v(\"如第一步\"),_(\"code\",[v._v(\"I\")]),v._v(\"最优，那我们直接确定第一个字幕为\"),_(\"code\",[v._v(\"I\")]),v._v(\"；第二步\"),_(\"code\",[v._v(\"H\")]),v._v(\"最优，同样确定下来；第三步\"),_(\"code\",[v._v(\"U\")]),v._v(\"最优，完成搜索\")]),v._v(\" \"),_(\"p\",[v._v(\"贪心算法本质上没有从整体最优上加以考虑，并不能保证最终的结果一定是全局最优的。但是相对穷举搜索，搜索效率大大提升\")]),v._v(\" \"),_(\"h4\",{attrs:{id:\"beam-search-2\"}},[_(\"a\",{staticClass:\"header-anchor\",attrs:{href:\"#beam-search-2\"}},[v._v(\"#\")]),v._v(\" Beam Search\")]),v._v(\" \"),_(\"p\",[_(\"code\",[v._v(\"beam search\")]),v._v(\"是对\"),_(\"code\",[v._v(\"greedy search\")]),v._v(\"的一个改进算法。相对\"),_(\"code\",[v._v(\"greedy search\")]),v._v(\"扩大了搜索空间，但远远不及穷举搜索指数级的搜索空间，是二者的一个折中方案\")]),v._v(\" \"),_(\"p\",[_(\"code\",[v._v(\"beam search\")]),v._v(\"有一个参数\"),_(\"code\",[v._v(\"beam size\")]),v._v(\"（束宽），设为\"),_(\"code\",[v._v(\"K\")]),v._v(\"，它的搜索过程如下\")]),v._v(\" \"),_(\"ul\",[_(\"li\",[v._v(\"在首次选取节点进行扩展时，留下条件概率最高的\"),_(\"code\",[v._v(\"K\")]),v._v(\"个节点进行扩展，剩下的节点被修剪掉\")]),v._v(\" \"),_(\"li\",[v._v(\"之后的每次选取，基于上次选取的高概率项，挑选出所有组合中条件概率最大的\"),_(\"code\",[v._v(\"K\")]),v._v(\"个，作为下次选取的候选序列\")]),v._v(\" \"),_(\"li\",[v._v(\"就这样始终保持\"),_(\"code\",[v._v(\"K\")]),v._v(\"个候选。直到遍历到叶子，在最后的\"),_(\"code\",[v._v(\"K\")]),v._v(\"个节点中选择概率最高的作为搜索结果\")])]),v._v(\" \"),_(\"p\",[v._v(\"注意：\")]),v._v(\" \"),_(\"p\",[_(\"code\",[v._v(\"beam search\")]),v._v(\"不保证全局最优，但是比\"),_(\"code\",[v._v(\"greedy search\")]),v._v(\"搜索空间更大，一般结果比\"),_(\"code\",[v._v(\"greedy search\")]),v._v(\"要好\")]),v._v(\" \"),_(\"p\",[_(\"code\",[v._v(\"greedy search\")]),v._v(\"可以看做是\"),_(\"code\",[v._v(\"beam size = 1\")]),v._v(\"时的\"),_(\"code\",[v._v(\"beam search\")])]),v._v(\" \"),_(\"h3\",{attrs:{id:\"genetic-algorithm\"}},[_(\"a\",{staticClass:\"header-anchor\",attrs:{href:\"#genetic-algorithm\"}},[v._v(\"#\")]),v._v(\" Genetic Algorithm\")]),v._v(\" \"),_(\"blockquote\",[_(\"p\",[v._v(\"GA：遗传算法\")])]),v._v(\" \"),_(\"p\",[_(\"a\",{attrs:{href:\"https://zhuanlan.zhihu.com/p/100337680\",target:\"_blank\",rel:\"noopener noreferrer\"}},[v._v(\"遗传算法入门详解 - 知乎\"),_(\"OutboundLink\")],1)]),v._v(\" \"),_(\"p\",[_(\"a\",{attrs:{href:\"https://www.jianshu.com/p/ae5157c26af9\",target:\"_blank\",rel:\"noopener noreferrer\"}},[v._v(\"遗传算法(Genetic Algorithm)解析 - 简书\"),_(\"OutboundLink\")],1)]),v._v(\" \"),_(\"h3\",{attrs:{id:\"iterative-deepening-a\"}},[_(\"a\",{staticClass:\"header-anchor\",attrs:{href:\"#iterative-deepening-a\"}},[v._v(\"#\")]),v._v(\" Iterative Deepening A*\")]),v._v(\" \"),_(\"blockquote\",[_(\"p\",[v._v(\"迭代加深的A*算法\")]),v._v(\" \"),_(\"p\",[v._v(\"A*搜索把状态视为内部无结构的黑盒\")])]),v._v(\" \"),_(\"p\",[v._v(\"A*算法和迭代加深的深度优先搜索的结合\")]),v._v(\" \"),_(\"h3\",{attrs:{id:\"hmm\"}},[_(\"a\",{staticClass:\"header-anchor\",attrs:{href:\"#hmm\"}},[v._v(\"#\")]),v._v(\" HMM\")]),v._v(\" \"),_(\"blockquote\",[_(\"p\",[v._v(\"隐马尔可夫模型\")]),v._v(\" \"),_(\"p\",[v._v(\"属于时序模型\")])]),v._v(\" \"),_(\"p\",[_(\"a\",{attrs:{href:\"https://www.cnblogs.com/skyme/p/4651331.html\",target:\"_blank\",rel:\"noopener noreferrer\"}},[v._v(\"一文搞懂HMM（隐马尔可夫模型）skyme  博客园\"),_(\"OutboundLink\")],1)])])}),[],!1,null,null,null);_.default=a.exports}}]);","extractedComments":[]}